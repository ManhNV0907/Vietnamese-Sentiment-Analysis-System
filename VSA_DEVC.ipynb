{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VSA_DEVC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXFgLewuoj4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b8e1ab-65a6-467f-b0cd-d28ec28fd484"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Nov 21 03:39:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOsL4cT0rwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2352dc9-cb13-44eb-97c5-47d57dc261b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTa3fdD-YJEL"
      },
      "source": [
        "!cp /content/drive/MyDrive/baomoi.window2.vn.model.bin.gz /content/baomoi.window2.vn.model.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4usneVJGezW"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj6uXMpgGEM1"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(\"/content/baomoi.window2.vn.model.bin.gz\", binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzr9Ntom9Fi6"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivL308tK9L_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc263597-2047-4f58-db1e-1b7200e34f3e"
      },
      "source": [
        "data = pd.read_csv(\"/content/DEVC_DATA.csv\")\n",
        "data = data.dropna()\n",
        "labels = [int(x) for x in data.label.values]\n",
        "positive_label = 0\n",
        "negative_label = 0\n",
        "neural_label = 0\n",
        "for label in labels:\n",
        "  if label == 0:\n",
        "    positive_label += 1\n",
        "  elif label == 2:\n",
        "    negative_label += 1\n",
        "  else:\n",
        "    neural_label += 1  \n",
        "print(set(labels))\n",
        "print(positive_label)\n",
        "print(negative_label)\n",
        "print(neural_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2}\n",
            "45727\n",
            "13367\n",
            "19764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQMdDxNe31HX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b771faa-3cd9-4a2a-fe26-372559fa3fb8"
      },
      "source": [
        "train, test = train_test_split(data, test_size = 0.3, random_state = 42)\n",
        "train[\"label\"] = train[\"label\"].apply(lambda x: int(x))\n",
        "test[\"label\"] = test[\"label\"].apply(lambda x: int(x))\n",
        "train.to_csv(\"/content/train.csv\", index=False)\n",
        "test.to_csv(\"/content/test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFTgsJ5AcVJp"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMyGZemOpYC6"
      },
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.int)\n",
        "text_field = Field(sequential=True, include_lengths=True, use_vocab=True, batch_first=True)\n",
        "fields = [(\"text\", text_field),(\"label\", label_field)]\n",
        "train, test = TabularDataset.splits(path='/content/', train='train.csv', test='test.csv',\n",
        "                                           format='CSV', fields=fields, skip_header=True)\n",
        "train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "text_field.build_vocab(train, min_freq=2)\n",
        "vocab_size = len(text_field.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2cTSd2c5Pxh"
      },
      "source": [
        "import pickle\n",
        "def save_vocab(vocab, path):\n",
        "    vocab_dict = dict()\n",
        "    vocab_file = open(path, \"wb\")\n",
        "    for token, index in vocab.stoi.items():\n",
        "        vocab_dict[token] = index\n",
        "    pickle.dump(vocab_dict, vocab_file)\n",
        "def read_vocab(path):\n",
        "    vocab_file = open(path, \"rb\")\n",
        "    vocab = pickle.load(vocab_file)\n",
        "    return vocab\n",
        "def predict_sentiment(model, sentence, vocab, unk_idx):\n",
        "\n",
        "    tokens = tokenize(sentence) #convert string to tokens, needs to be same tokenization as training\n",
        "    indexes = [vocab.get(t, unk_idx) for t in tokens] #converts to index or unk if not in vocab\n",
        "    tensor = torch.LongTensor(indexes).unsqueeze(1) #convert to tensor and add batch dimension\n",
        "    output = model(tensor) #get output from model\n",
        "    prediction = torch.sigmoid(output) #squeeze between 0-1 range\n",
        "    return prediction\n",
        "save_vocab(text_field.vocab, \"/content/vocab.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZr8NMg7BT5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889b69a1-a516-4381-cbb9-30ef9b2b9b8f"
      },
      "source": [
        "vocab = read_vocab(\"/content/vocab.pkl\")\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKDNG45UKIc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a426dc-16bf-40cd-c81b-69bf1d6f3169"
      },
      "source": [
        "pretrained_vectors = []\n",
        "for token, id in text_field.vocab.stoi.items():\n",
        "  if token in model.vocab.keys():\n",
        "    pretrained_vectors.append(torch.FloatTensor(torch.from_numpy(model[token])))\n",
        "  else:\n",
        "    pretrained_vectors.append(torch.zeros(300))\n",
        "text_field.vocab.set_vectors(text_field.vocab.stoi, pretrained_vectors, 300)\n",
        "pretrained_embedding = torch.FloatTensor(text_field.vocab.vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiWq0WCX4uJa"
      },
      "source": [
        "class VSA_BiLSTM(nn.Module):\n",
        "  def __init__(self, pretrained_emb, emb_size=300, hidden_size=512):\n",
        "    super(VSA_BiLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
        "    # self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.lstm = nn.LSTM(input_size=emb_size,\n",
        "                        hidden_size=self.hidden_size,\n",
        "                        num_layers=1,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=True)\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.fc = nn.Linear(2*self.hidden_size,3)\n",
        "  def forward(self, text):\n",
        "    #Text = [Batch_size, seq_len]\n",
        "    text_emb = self.embedding(text)\n",
        "    #Text_emb = [Batch_size, seq_len, emb_dim]\n",
        "    output, (hidden, cell) = self.lstm(text_emb)\n",
        "    #output = [Batch_Size, seq_len, hidden_size*directions]\n",
        "    output_forward = torch.squeeze(output[:, -1, :self.hidden_size], 1)\n",
        "    output_backward = torch.squeeze(output[:, -1, self.hidden_size:], 1)\n",
        "    out_bidirectional = torch.cat((output_forward, output_backward), 1)\n",
        "    out_dropped = self.dropout(out_bidirectional)\n",
        "    return self.fc(out_dropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZkIVVaHWD6q"
      },
      "source": [
        "class VSA_BiLSTM_sa(nn.Module):\n",
        "  def __init__(self, pretrained_emb, emb_size=300, hidden_size=512, d_a=350, r=30):\n",
        "    super(VSA_BiLSTM_sa, self).__init__()\n",
        "    # self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.r = r\n",
        "    self.d_a = d_a\n",
        "    self.lstm = nn.LSTM(input_size=emb_size,\n",
        "                        hidden_size=self.hidden_size,\n",
        "                        num_layers=1,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=True)\n",
        "    self.linear_first = torch.nn.Linear(self.hidden_size*2,self.d_a)\n",
        "    self.linear_first.bias.data.fill_(0)\n",
        "    self.linear_second = torch.nn.Linear(self.d_a,self.r)\n",
        "    self.linear_second.bias.data.fill_(0)\n",
        "    self.linear_final = torch.nn.Linear(self.hidden_size*2,3)\n",
        "\n",
        "  def forward(self, text):\n",
        "    #Text = [Batch_size, seq_len]\n",
        "    text_emb = self.embedding(text)\n",
        "    #Text_emb = [Batch_size, seq_len, emb_dim]\n",
        "    output, (hidden, cell) = self.lstm(text_emb)\n",
        "    #output = [Batch_Size, seq_len, hidden_size*directions]\n",
        "    x = F.tanh(self.linear_first(output))\n",
        "    #x = [Batch_size, seq_len, d_a]\n",
        "    x = self.linear_second(x)\n",
        "    #x = [Batch_size, seq_len, r]\n",
        "    x = self.softmax(x, axis = 1)\n",
        "    #x = [Batch_size, seq_len, r]\n",
        "    attention = x.transpose(1,2)   \n",
        "    #x = [Batch_size, r, seq_len]    \n",
        "    sentence_embeddings = attention@output\n",
        "    #x = [Batch_size, r, hiddensize*2]    \n",
        "    avg_sentence_embeddings = torch.sum(sentence_embeddings,1)/self.r\n",
        "    return self.linear_final(avg_sentence_embeddings)\n",
        "    \n",
        "    \n",
        "\n",
        "  def softmax(self,input, axis=1):\n",
        "\n",
        "      input_size = input.size()\n",
        "      trans_input = input.transpose(axis, len(input_size)-1)\n",
        "      trans_size = trans_input.size()\n",
        "      input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
        "      soft_max_2d = F.softmax(input_2d)\n",
        "      soft_max_nd = soft_max_2d.view(*trans_size)\n",
        "      return soft_max_nd.transpose(axis, len(input_size)-1)\n",
        "                                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dww0ov5mXplg"
      },
      "source": [
        "Dataloader = {\"train\": train_iter, \"val\": test_iter}\n",
        "vsa_bilstm_sa = VSA_BiLSTM_sa(pretrained_embedding).to(device)\n",
        "optimizer = optim.Adam(vsa_bilstm_sa.parameters(), lr=0.0005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GknVkKTlSH0y"
      },
      "source": [
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "total_step = len(Dataloader[\"train\"])\n",
        "def train_model(model, optimizer, device, num_epochs=300):\n",
        "    since = time.time()\n",
        "    print(\"Start training process\")\n",
        "    for epoch in range(1,num_epochs + 1):\n",
        "        loss_set = []\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        Pred_lst = []\n",
        "        Label_lst = []\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == 'train':\n",
        "                model.train()  \n",
        "            else:\n",
        "                model.eval()   \n",
        "            for i, batch in enumerate(Dataloader[phase]):\n",
        "                texts, texts_len = batch.text\n",
        "                len_sen = texts.size()[1]\n",
        "                # if len_sen <= 5:\n",
        "                #   pad = torch.zeros((32, 5-len_sen))\n",
        "                #   texts = torch.cat((texts.cuda(), pad.cuda()), 1)\n",
        "                labels = batch.label\n",
        "                texts = texts.long().to(device)\n",
        "                labels = labels.long().to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(texts)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "                    if phase == \"val\":\n",
        "                      prediction = torch.argmax(outputs, dim = 1)\n",
        "                      for ele in prediction.detach().cpu().numpy():\n",
        "                        Pred_lst.append(ele)\n",
        "                      for ele in labels.detach().cpu().numpy():\n",
        "                        Label_lst.append(ele)\n",
        "                    if phase == 'train':\n",
        "                        loss_set.append(loss.item())\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                if (i % 50) == 0:\n",
        "                  print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch, num_epochs, i+1, total_step, loss.item()))\n",
        "            if phase == \"train\":\n",
        "              print(\" Training process of epoch {} has been done  ------- Loss_train: {}\".format(epoch, loss_set[-1]))\n",
        "            elif phase == \"val\":\n",
        "              Pred_lst = [int(ele) for ele in Pred_lst]\n",
        "              Label_lst = [int(ele) for ele in Label_lst]\n",
        "              print(\" Validation process of epoch {} has been done -- F1_score_val: {}% -- Acc_score: {}%\".format(epoch, 100*f1_score(Pred_lst, Label_lst, labels=[0,1,2], average=\"weighted\"), 100*accuracy_score(Label_lst, Pred_lst)))         \n",
        "        if False:\n",
        "          path = \"/content/vsa_lstm_sa/\" + \"BiLSTM_Classification_\" + str(epoch) + \".pth\"\n",
        "          torch.save(model.state_dict(), path)   \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh45mdfi9v-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d62fc9c-02c3-477a-8ba0-1df486f8b3e0"
      },
      "source": [
        "train_model(vsa_bilstm_sa, optimizer, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training process\n",
            "Epoch 1/300\n",
            "----------\n",
            "Epoch [1/300], Step [1/1725], Loss: 1.1042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/300], Step [51/1725], Loss: 0.9764\n",
            "Epoch [1/300], Step [101/1725], Loss: 0.6900\n",
            "Epoch [1/300], Step [151/1725], Loss: 0.4212\n",
            "Epoch [1/300], Step [201/1725], Loss: 0.6184\n",
            "Epoch [1/300], Step [251/1725], Loss: 0.4079\n",
            "Epoch [1/300], Step [301/1725], Loss: 0.4783\n",
            "Epoch [1/300], Step [351/1725], Loss: 0.7679\n",
            "Epoch [1/300], Step [401/1725], Loss: 0.6810\n",
            "Epoch [1/300], Step [451/1725], Loss: 0.5477\n",
            "Epoch [1/300], Step [501/1725], Loss: 0.6384\n",
            "Epoch [1/300], Step [551/1725], Loss: 0.6625\n",
            "Epoch [1/300], Step [601/1725], Loss: 0.6591\n",
            "Epoch [1/300], Step [651/1725], Loss: 0.4910\n",
            "Epoch [1/300], Step [701/1725], Loss: 0.6910\n",
            "Epoch [1/300], Step [751/1725], Loss: 0.5837\n",
            "Epoch [1/300], Step [801/1725], Loss: 0.5230\n",
            "Epoch [1/300], Step [851/1725], Loss: 0.7867\n",
            "Epoch [1/300], Step [901/1725], Loss: 0.8356\n",
            "Epoch [1/300], Step [951/1725], Loss: 0.6639\n",
            "Epoch [1/300], Step [1001/1725], Loss: 0.6987\n",
            "Epoch [1/300], Step [1051/1725], Loss: 0.6169\n",
            "Epoch [1/300], Step [1101/1725], Loss: 0.6888\n",
            "Epoch [1/300], Step [1151/1725], Loss: 0.6894\n",
            "Epoch [1/300], Step [1201/1725], Loss: 0.8233\n",
            "Epoch [1/300], Step [1251/1725], Loss: 0.5328\n",
            "Epoch [1/300], Step [1301/1725], Loss: 0.5036\n",
            "Epoch [1/300], Step [1351/1725], Loss: 0.6976\n",
            "Epoch [1/300], Step [1401/1725], Loss: 0.6212\n",
            "Epoch [1/300], Step [1451/1725], Loss: 0.5659\n",
            "Epoch [1/300], Step [1501/1725], Loss: 0.5869\n",
            "Epoch [1/300], Step [1551/1725], Loss: 0.6269\n",
            "Epoch [1/300], Step [1601/1725], Loss: 0.6693\n",
            "Epoch [1/300], Step [1651/1725], Loss: 0.4486\n",
            "Epoch [1/300], Step [1701/1725], Loss: 0.3456\n",
            " Training process of epoch 1 has been done  ------- Loss_train: 0.6047007441520691\n",
            "Epoch [1/300], Step [1/1725], Loss: 0.6451\n",
            "Epoch [1/300], Step [51/1725], Loss: 0.7864\n",
            "Epoch [1/300], Step [101/1725], Loss: 0.8519\n",
            "Epoch [1/300], Step [151/1725], Loss: 0.7616\n",
            "Epoch [1/300], Step [201/1725], Loss: 0.9595\n",
            "Epoch [1/300], Step [251/1725], Loss: 0.8368\n",
            "Epoch [1/300], Step [301/1725], Loss: 0.6588\n",
            "Epoch [1/300], Step [351/1725], Loss: 0.8244\n",
            "Epoch [1/300], Step [401/1725], Loss: 1.0242\n",
            "Epoch [1/300], Step [451/1725], Loss: 0.6737\n",
            "Epoch [1/300], Step [501/1725], Loss: 0.5908\n",
            "Epoch [1/300], Step [551/1725], Loss: 0.6187\n",
            "Epoch [1/300], Step [601/1725], Loss: 0.3982\n",
            "Epoch [1/300], Step [651/1725], Loss: 0.2733\n",
            "Epoch [1/300], Step [701/1725], Loss: 0.3013\n",
            " Validation process of epoch 1 has been done -- F1_score_val: 72.3680099434512% -- Acc_score: 72.82103305435793%\n",
            "Epoch 2/300\n",
            "----------\n",
            "Epoch [2/300], Step [1/1725], Loss: 0.5503\n",
            "Epoch [2/300], Step [51/1725], Loss: 0.8491\n",
            "Epoch [2/300], Step [101/1725], Loss: 0.6580\n",
            "Epoch [2/300], Step [151/1725], Loss: 0.3272\n",
            "Epoch [2/300], Step [201/1725], Loss: 0.3812\n",
            "Epoch [2/300], Step [251/1725], Loss: 0.2138\n",
            "Epoch [2/300], Step [301/1725], Loss: 0.3161\n",
            "Epoch [2/300], Step [351/1725], Loss: 0.6133\n",
            "Epoch [2/300], Step [401/1725], Loss: 0.4954\n",
            "Epoch [2/300], Step [451/1725], Loss: 0.3042\n",
            "Epoch [2/300], Step [501/1725], Loss: 0.4904\n",
            "Epoch [2/300], Step [551/1725], Loss: 0.4214\n",
            "Epoch [2/300], Step [601/1725], Loss: 0.3561\n",
            "Epoch [2/300], Step [651/1725], Loss: 0.3078\n",
            "Epoch [2/300], Step [701/1725], Loss: 0.5750\n",
            "Epoch [2/300], Step [751/1725], Loss: 0.4062\n",
            "Epoch [2/300], Step [801/1725], Loss: 0.4027\n",
            "Epoch [2/300], Step [851/1725], Loss: 0.5465\n",
            "Epoch [2/300], Step [901/1725], Loss: 0.4836\n",
            "Epoch [2/300], Step [951/1725], Loss: 0.2991\n",
            "Epoch [2/300], Step [1001/1725], Loss: 0.4245\n",
            "Epoch [2/300], Step [1051/1725], Loss: 0.3796\n",
            "Epoch [2/300], Step [1101/1725], Loss: 0.5604\n",
            "Epoch [2/300], Step [1151/1725], Loss: 0.4111\n",
            "Epoch [2/300], Step [1201/1725], Loss: 0.6760\n",
            "Epoch [2/300], Step [1251/1725], Loss: 0.3424\n",
            "Epoch [2/300], Step [1301/1725], Loss: 0.3878\n",
            "Epoch [2/300], Step [1351/1725], Loss: 0.4999\n",
            "Epoch [2/300], Step [1401/1725], Loss: 0.4590\n",
            "Epoch [2/300], Step [1451/1725], Loss: 0.4583\n",
            "Epoch [2/300], Step [1501/1725], Loss: 0.4797\n",
            "Epoch [2/300], Step [1551/1725], Loss: 0.5351\n",
            "Epoch [2/300], Step [1601/1725], Loss: 0.4894\n",
            "Epoch [2/300], Step [1651/1725], Loss: 0.2848\n",
            "Epoch [2/300], Step [1701/1725], Loss: 0.2280\n",
            " Training process of epoch 2 has been done  ------- Loss_train: 0.4601617753505707\n",
            "Epoch [2/300], Step [1/1725], Loss: 0.7184\n",
            "Epoch [2/300], Step [51/1725], Loss: 0.8036\n",
            "Epoch [2/300], Step [101/1725], Loss: 0.7196\n",
            "Epoch [2/300], Step [151/1725], Loss: 0.8275\n",
            "Epoch [2/300], Step [201/1725], Loss: 0.6118\n",
            "Epoch [2/300], Step [251/1725], Loss: 0.8952\n",
            "Epoch [2/300], Step [301/1725], Loss: 0.5640\n",
            "Epoch [2/300], Step [351/1725], Loss: 0.5848\n",
            "Epoch [2/300], Step [401/1725], Loss: 0.8393\n",
            "Epoch [2/300], Step [451/1725], Loss: 0.7114\n",
            "Epoch [2/300], Step [501/1725], Loss: 0.5484\n",
            "Epoch [2/300], Step [551/1725], Loss: 0.4631\n",
            "Epoch [2/300], Step [601/1725], Loss: 0.2837\n",
            "Epoch [2/300], Step [651/1725], Loss: 0.2696\n",
            "Epoch [2/300], Step [701/1725], Loss: 0.3504\n",
            " Validation process of epoch 2 has been done -- F1_score_val: 76.45262955402885% -- Acc_score: 76.68019274663962%\n",
            "Epoch 3/300\n",
            "----------\n",
            "Epoch [3/300], Step [1/1725], Loss: 0.7305\n",
            "Epoch [3/300], Step [51/1725], Loss: 0.6841\n",
            "Epoch [3/300], Step [101/1725], Loss: 0.6522\n",
            "Epoch [3/300], Step [151/1725], Loss: 0.2115\n",
            "Epoch [3/300], Step [201/1725], Loss: 0.3054\n",
            "Epoch [3/300], Step [251/1725], Loss: 0.1307\n",
            "Epoch [3/300], Step [301/1725], Loss: 0.2281\n",
            "Epoch [3/300], Step [351/1725], Loss: 0.4820\n",
            "Epoch [3/300], Step [401/1725], Loss: 0.3817\n",
            "Epoch [3/300], Step [451/1725], Loss: 0.2271\n",
            "Epoch [3/300], Step [501/1725], Loss: 0.2616\n",
            "Epoch [3/300], Step [551/1725], Loss: 0.2180\n",
            "Epoch [3/300], Step [601/1725], Loss: 0.2217\n",
            "Epoch [3/300], Step [651/1725], Loss: 0.1631\n",
            "Epoch [3/300], Step [701/1725], Loss: 0.4149\n",
            "Epoch [3/300], Step [751/1725], Loss: 0.2194\n",
            "Epoch [3/300], Step [801/1725], Loss: 0.3090\n",
            "Epoch [3/300], Step [851/1725], Loss: 0.3342\n",
            "Epoch [3/300], Step [901/1725], Loss: 0.2338\n",
            "Epoch [3/300], Step [951/1725], Loss: 0.1417\n",
            "Epoch [3/300], Step [1001/1725], Loss: 0.2219\n",
            "Epoch [3/300], Step [1051/1725], Loss: 0.2775\n",
            "Epoch [3/300], Step [1101/1725], Loss: 0.3271\n",
            "Epoch [3/300], Step [1151/1725], Loss: 0.2355\n",
            "Epoch [3/300], Step [1201/1725], Loss: 0.3915\n",
            "Epoch [3/300], Step [1251/1725], Loss: 0.2099\n",
            "Epoch [3/300], Step [1301/1725], Loss: 0.2258\n",
            "Epoch [3/300], Step [1351/1725], Loss: 0.1991\n",
            "Epoch [3/300], Step [1401/1725], Loss: 0.2481\n",
            "Epoch [3/300], Step [1451/1725], Loss: 0.1519\n",
            "Epoch [3/300], Step [1501/1725], Loss: 0.2820\n",
            "Epoch [3/300], Step [1551/1725], Loss: 0.4152\n",
            "Epoch [3/300], Step [1601/1725], Loss: 0.2634\n",
            "Epoch [3/300], Step [1651/1725], Loss: 0.2128\n",
            "Epoch [3/300], Step [1701/1725], Loss: 0.0764\n",
            " Training process of epoch 3 has been done  ------- Loss_train: 0.30141034722328186\n",
            "Epoch [3/300], Step [1/1725], Loss: 0.8553\n",
            "Epoch [3/300], Step [51/1725], Loss: 0.9464\n",
            "Epoch [3/300], Step [101/1725], Loss: 0.7784\n",
            "Epoch [3/300], Step [151/1725], Loss: 0.8563\n",
            "Epoch [3/300], Step [201/1725], Loss: 0.5639\n",
            "Epoch [3/300], Step [251/1725], Loss: 1.0146\n",
            "Epoch [3/300], Step [301/1725], Loss: 0.6465\n",
            "Epoch [3/300], Step [351/1725], Loss: 0.7134\n",
            "Epoch [3/300], Step [401/1725], Loss: 1.0573\n",
            "Epoch [3/300], Step [451/1725], Loss: 0.9045\n",
            "Epoch [3/300], Step [501/1725], Loss: 0.7425\n",
            "Epoch [3/300], Step [551/1725], Loss: 0.2786\n",
            "Epoch [3/300], Step [601/1725], Loss: 0.2267\n",
            "Epoch [3/300], Step [651/1725], Loss: 0.3377\n",
            "Epoch [3/300], Step [701/1725], Loss: 0.4300\n",
            " Validation process of epoch 3 has been done -- F1_score_val: 79.12317221355084% -- Acc_score: 79.24592104150815%\n",
            "Epoch 4/300\n",
            "----------\n",
            "Epoch [4/300], Step [1/1725], Loss: 0.7766\n",
            "Epoch [4/300], Step [51/1725], Loss: 0.5675\n",
            "Epoch [4/300], Step [101/1725], Loss: 0.5185\n",
            "Epoch [4/300], Step [151/1725], Loss: 0.1516\n",
            "Epoch [4/300], Step [201/1725], Loss: 0.2755\n",
            "Epoch [4/300], Step [251/1725], Loss: 0.0899\n",
            "Epoch [4/300], Step [301/1725], Loss: 0.1780\n",
            "Epoch [4/300], Step [351/1725], Loss: 0.2517\n",
            "Epoch [4/300], Step [401/1725], Loss: 0.2569\n",
            "Epoch [4/300], Step [451/1725], Loss: 0.0590\n",
            "Epoch [4/300], Step [501/1725], Loss: 0.1775\n",
            "Epoch [4/300], Step [551/1725], Loss: 0.1100\n",
            "Epoch [4/300], Step [601/1725], Loss: 0.0672\n",
            "Epoch [4/300], Step [651/1725], Loss: 0.0947\n",
            "Epoch [4/300], Step [701/1725], Loss: 0.4263\n",
            "Epoch [4/300], Step [751/1725], Loss: 0.1340\n",
            "Epoch [4/300], Step [801/1725], Loss: 0.2289\n",
            "Epoch [4/300], Step [851/1725], Loss: 0.0975\n",
            "Epoch [4/300], Step [901/1725], Loss: 0.1045\n",
            "Epoch [4/300], Step [951/1725], Loss: 0.0513\n",
            "Epoch [4/300], Step [1001/1725], Loss: 0.2071\n",
            "Epoch [4/300], Step [1051/1725], Loss: 0.2610\n",
            "Epoch [4/300], Step [1101/1725], Loss: 0.1580\n",
            "Epoch [4/300], Step [1151/1725], Loss: 0.1100\n",
            "Epoch [4/300], Step [1201/1725], Loss: 0.2231\n",
            "Epoch [4/300], Step [1251/1725], Loss: 0.0623\n",
            "Epoch [4/300], Step [1301/1725], Loss: 0.1122\n",
            "Epoch [4/300], Step [1351/1725], Loss: 0.1012\n",
            "Epoch [4/300], Step [1401/1725], Loss: 0.0667\n",
            "Epoch [4/300], Step [1451/1725], Loss: 0.0620\n",
            "Epoch [4/300], Step [1501/1725], Loss: 0.1716\n",
            "Epoch [4/300], Step [1551/1725], Loss: 0.1638\n",
            "Epoch [4/300], Step [1601/1725], Loss: 0.1002\n",
            "Epoch [4/300], Step [1651/1725], Loss: 0.3478\n",
            "Epoch [4/300], Step [1701/1725], Loss: 0.0196\n",
            " Training process of epoch 4 has been done  ------- Loss_train: 0.30752378702163696\n",
            "Epoch [4/300], Step [1/1725], Loss: 0.8473\n",
            "Epoch [4/300], Step [51/1725], Loss: 0.9663\n",
            "Epoch [4/300], Step [101/1725], Loss: 0.8923\n",
            "Epoch [4/300], Step [151/1725], Loss: 0.9857\n",
            "Epoch [4/300], Step [201/1725], Loss: 0.3704\n",
            "Epoch [4/300], Step [251/1725], Loss: 1.0073\n",
            "Epoch [4/300], Step [301/1725], Loss: 0.9428\n",
            "Epoch [4/300], Step [351/1725], Loss: 0.8258\n",
            "Epoch [4/300], Step [401/1725], Loss: 1.2074\n",
            "Epoch [4/300], Step [451/1725], Loss: 0.7878\n",
            "Epoch [4/300], Step [501/1725], Loss: 0.8067\n",
            "Epoch [4/300], Step [551/1725], Loss: 0.8085\n",
            "Epoch [4/300], Step [601/1725], Loss: 0.6543\n",
            "Epoch [4/300], Step [651/1725], Loss: 0.3257\n",
            "Epoch [4/300], Step [701/1725], Loss: 0.7838\n",
            " Validation process of epoch 4 has been done -- F1_score_val: 78.1229879068634% -- Acc_score: 78.37940654324119%\n",
            "Epoch 5/300\n",
            "----------\n",
            "Epoch [5/300], Step [1/1725], Loss: 1.0101\n",
            "Epoch [5/300], Step [51/1725], Loss: 0.5197\n",
            "Epoch [5/300], Step [101/1725], Loss: 0.4731\n",
            "Epoch [5/300], Step [151/1725], Loss: 0.1094\n",
            "Epoch [5/300], Step [201/1725], Loss: 0.3830\n",
            "Epoch [5/300], Step [251/1725], Loss: 0.0485\n",
            "Epoch [5/300], Step [301/1725], Loss: 0.1202\n",
            "Epoch [5/300], Step [351/1725], Loss: 0.1486\n",
            "Epoch [5/300], Step [401/1725], Loss: 0.1898\n",
            "Epoch [5/300], Step [451/1725], Loss: 0.0586\n",
            "Epoch [5/300], Step [501/1725], Loss: 0.1014\n",
            "Epoch [5/300], Step [551/1725], Loss: 0.0787\n",
            "Epoch [5/300], Step [601/1725], Loss: 0.0517\n",
            "Epoch [5/300], Step [651/1725], Loss: 0.0278\n",
            "Epoch [5/300], Step [701/1725], Loss: 0.4364\n",
            "Epoch [5/300], Step [751/1725], Loss: 0.0664\n",
            "Epoch [5/300], Step [801/1725], Loss: 0.2285\n",
            "Epoch [5/300], Step [851/1725], Loss: 0.0844\n",
            "Epoch [5/300], Step [901/1725], Loss: 0.0745\n",
            "Epoch [5/300], Step [951/1725], Loss: 0.0324\n",
            "Epoch [5/300], Step [1001/1725], Loss: 0.1222\n",
            "Epoch [5/300], Step [1051/1725], Loss: 0.0822\n",
            "Epoch [5/300], Step [1101/1725], Loss: 0.1167\n",
            "Epoch [5/300], Step [1151/1725], Loss: 0.0260\n",
            "Epoch [5/300], Step [1201/1725], Loss: 0.1858\n",
            "Epoch [5/300], Step [1251/1725], Loss: 0.0499\n",
            "Epoch [5/300], Step [1301/1725], Loss: 0.0436\n",
            "Epoch [5/300], Step [1351/1725], Loss: 0.1120\n",
            "Epoch [5/300], Step [1401/1725], Loss: 0.0341\n",
            "Epoch [5/300], Step [1451/1725], Loss: 0.0293\n",
            "Epoch [5/300], Step [1501/1725], Loss: 0.0911\n",
            "Epoch [5/300], Step [1551/1725], Loss: 0.2081\n",
            "Epoch [5/300], Step [1601/1725], Loss: 0.0773\n",
            "Epoch [5/300], Step [1651/1725], Loss: 0.1760\n",
            "Epoch [5/300], Step [1701/1725], Loss: 0.0130\n",
            " Training process of epoch 5 has been done  ------- Loss_train: 0.2553529441356659\n",
            "Epoch [5/300], Step [1/1725], Loss: 0.9131\n",
            "Epoch [5/300], Step [51/1725], Loss: 0.8819\n",
            "Epoch [5/300], Step [101/1725], Loss: 1.0277\n",
            "Epoch [5/300], Step [151/1725], Loss: 0.8822\n",
            "Epoch [5/300], Step [201/1725], Loss: 0.3472\n",
            "Epoch [5/300], Step [251/1725], Loss: 0.7902\n",
            "Epoch [5/300], Step [301/1725], Loss: 0.9519\n",
            "Epoch [5/300], Step [351/1725], Loss: 0.7361\n",
            "Epoch [5/300], Step [401/1725], Loss: 1.4735\n",
            "Epoch [5/300], Step [451/1725], Loss: 1.1657\n",
            "Epoch [5/300], Step [501/1725], Loss: 0.5774\n",
            "Epoch [5/300], Step [551/1725], Loss: 0.4235\n",
            "Epoch [5/300], Step [601/1725], Loss: 0.3189\n",
            "Epoch [5/300], Step [651/1725], Loss: 0.3002\n",
            "Epoch [5/300], Step [701/1725], Loss: 0.4264\n",
            " Validation process of epoch 5 has been done -- F1_score_val: 81.35777332222885% -- Acc_score: 81.38473243723054%\n",
            "Epoch 6/300\n",
            "----------\n",
            "Epoch [6/300], Step [1/1725], Loss: 0.8998\n",
            "Epoch [6/300], Step [51/1725], Loss: 0.4736\n",
            "Epoch [6/300], Step [101/1725], Loss: 0.3295\n",
            "Epoch [6/300], Step [151/1725], Loss: 0.0874\n",
            "Epoch [6/300], Step [201/1725], Loss: 0.1485\n",
            "Epoch [6/300], Step [251/1725], Loss: 0.1489\n",
            "Epoch [6/300], Step [301/1725], Loss: 0.0544\n",
            "Epoch [6/300], Step [351/1725], Loss: 0.1341\n",
            "Epoch [6/300], Step [401/1725], Loss: 0.1845\n",
            "Epoch [6/300], Step [451/1725], Loss: 0.0520\n",
            "Epoch [6/300], Step [501/1725], Loss: 0.0326\n",
            "Epoch [6/300], Step [551/1725], Loss: 0.0490\n",
            "Epoch [6/300], Step [601/1725], Loss: 0.1436\n",
            "Epoch [6/300], Step [651/1725], Loss: 0.0118\n",
            "Epoch [6/300], Step [701/1725], Loss: 0.2847\n",
            "Epoch [6/300], Step [751/1725], Loss: 0.0402\n",
            "Epoch [6/300], Step [801/1725], Loss: 0.1120\n",
            "Epoch [6/300], Step [851/1725], Loss: 0.0460\n",
            "Epoch [6/300], Step [901/1725], Loss: 0.0160\n",
            "Epoch [6/300], Step [951/1725], Loss: 0.0415\n",
            "Epoch [6/300], Step [1001/1725], Loss: 0.1331\n",
            "Epoch [6/300], Step [1051/1725], Loss: 0.1520\n",
            "Epoch [6/300], Step [1101/1725], Loss: 0.0967\n",
            "Epoch [6/300], Step [1151/1725], Loss: 0.0169\n",
            "Epoch [6/300], Step [1201/1725], Loss: 0.1551\n",
            "Epoch [6/300], Step [1251/1725], Loss: 0.0341\n",
            "Epoch [6/300], Step [1301/1725], Loss: 0.0288\n",
            "Epoch [6/300], Step [1351/1725], Loss: 0.0403\n",
            "Epoch [6/300], Step [1401/1725], Loss: 0.0311\n",
            "Epoch [6/300], Step [1451/1725], Loss: 0.0543\n",
            "Epoch [6/300], Step [1501/1725], Loss: 0.1347\n",
            "Epoch [6/300], Step [1551/1725], Loss: 0.0825\n",
            "Epoch [6/300], Step [1601/1725], Loss: 0.0537\n",
            "Epoch [6/300], Step [1651/1725], Loss: 0.0613\n",
            "Epoch [6/300], Step [1701/1725], Loss: 0.0195\n",
            " Training process of epoch 6 has been done  ------- Loss_train: 0.16064779460430145\n",
            "Epoch [6/300], Step [1/1725], Loss: 0.9928\n",
            "Epoch [6/300], Step [51/1725], Loss: 0.8388\n",
            "Epoch [6/300], Step [101/1725], Loss: 0.9160\n",
            "Epoch [6/300], Step [151/1725], Loss: 0.9447\n",
            "Epoch [6/300], Step [201/1725], Loss: 0.4233\n",
            "Epoch [6/300], Step [251/1725], Loss: 0.7312\n",
            "Epoch [6/300], Step [301/1725], Loss: 0.9953\n",
            "Epoch [6/300], Step [351/1725], Loss: 0.9377\n",
            "Epoch [6/300], Step [401/1725], Loss: 1.3816\n",
            "Epoch [6/300], Step [451/1725], Loss: 1.2434\n",
            "Epoch [6/300], Step [501/1725], Loss: 0.5927\n",
            "Epoch [6/300], Step [551/1725], Loss: 0.4631\n",
            "Epoch [6/300], Step [601/1725], Loss: 0.3177\n",
            "Epoch [6/300], Step [651/1725], Loss: 0.3540\n",
            "Epoch [6/300], Step [701/1725], Loss: 0.2606\n",
            " Validation process of epoch 6 has been done -- F1_score_val: 82.6877066567624% -- Acc_score: 82.56826443486347%\n",
            "Epoch 7/300\n",
            "----------\n",
            "Epoch [7/300], Step [1/1725], Loss: 0.8398\n",
            "Epoch [7/300], Step [51/1725], Loss: 0.5191\n",
            "Epoch [7/300], Step [101/1725], Loss: 0.3107\n",
            "Epoch [7/300], Step [151/1725], Loss: 0.0568\n",
            "Epoch [7/300], Step [201/1725], Loss: 0.0709\n",
            "Epoch [7/300], Step [251/1725], Loss: 0.0673\n",
            "Epoch [7/300], Step [301/1725], Loss: 0.0297\n",
            "Epoch [7/300], Step [351/1725], Loss: 0.2219\n",
            "Epoch [7/300], Step [401/1725], Loss: 0.2375\n",
            "Epoch [7/300], Step [451/1725], Loss: 0.0842\n",
            "Epoch [7/300], Step [501/1725], Loss: 0.0241\n",
            "Epoch [7/300], Step [551/1725], Loss: 0.0358\n",
            "Epoch [7/300], Step [601/1725], Loss: 0.0449\n",
            "Epoch [7/300], Step [651/1725], Loss: 0.0097\n",
            "Epoch [7/300], Step [701/1725], Loss: 0.2737\n",
            "Epoch [7/300], Step [751/1725], Loss: 0.0142\n",
            "Epoch [7/300], Step [801/1725], Loss: 0.1316\n",
            "Epoch [7/300], Step [851/1725], Loss: 0.0287\n",
            "Epoch [7/300], Step [901/1725], Loss: 0.0082\n",
            "Epoch [7/300], Step [951/1725], Loss: 0.0138\n",
            "Epoch [7/300], Step [1001/1725], Loss: 0.1124\n",
            "Epoch [7/300], Step [1051/1725], Loss: 0.1023\n",
            "Epoch [7/300], Step [1101/1725], Loss: 0.0871\n",
            "Epoch [7/300], Step [1151/1725], Loss: 0.0309\n",
            "Epoch [7/300], Step [1201/1725], Loss: 0.0881\n",
            "Epoch [7/300], Step [1251/1725], Loss: 0.0637\n",
            "Epoch [7/300], Step [1301/1725], Loss: 0.0621\n",
            "Epoch [7/300], Step [1351/1725], Loss: 0.0324\n",
            "Epoch [7/300], Step [1401/1725], Loss: 0.0328\n",
            "Epoch [7/300], Step [1451/1725], Loss: 0.0604\n",
            "Epoch [7/300], Step [1501/1725], Loss: 0.0295\n",
            "Epoch [7/300], Step [1551/1725], Loss: 0.1725\n",
            "Epoch [7/300], Step [1601/1725], Loss: 0.0286\n",
            "Epoch [7/300], Step [1651/1725], Loss: 0.1021\n",
            "Epoch [7/300], Step [1701/1725], Loss: 0.0033\n",
            " Training process of epoch 7 has been done  ------- Loss_train: 0.18633045256137848\n",
            "Epoch [7/300], Step [1/1725], Loss: 0.9937\n",
            "Epoch [7/300], Step [51/1725], Loss: 0.8994\n",
            "Epoch [7/300], Step [101/1725], Loss: 0.9492\n",
            "Epoch [7/300], Step [151/1725], Loss: 0.9893\n",
            "Epoch [7/300], Step [201/1725], Loss: 0.3321\n",
            "Epoch [7/300], Step [251/1725], Loss: 0.8893\n",
            "Epoch [7/300], Step [301/1725], Loss: 1.0040\n",
            "Epoch [7/300], Step [351/1725], Loss: 0.8364\n",
            "Epoch [7/300], Step [401/1725], Loss: 1.4020\n",
            "Epoch [7/300], Step [451/1725], Loss: 1.0717\n",
            "Epoch [7/300], Step [501/1725], Loss: 0.7696\n",
            "Epoch [7/300], Step [551/1725], Loss: 0.2093\n",
            "Epoch [7/300], Step [601/1725], Loss: 0.5341\n",
            "Epoch [7/300], Step [651/1725], Loss: 0.4155\n",
            "Epoch [7/300], Step [701/1725], Loss: 0.4723\n",
            " Validation process of epoch 7 has been done -- F1_score_val: 83.30723875099353% -- Acc_score: 83.22343393355314%\n",
            "Epoch 8/300\n",
            "----------\n",
            "Epoch [8/300], Step [1/1725], Loss: 0.8494\n",
            "Epoch [8/300], Step [51/1725], Loss: 0.4404\n",
            "Epoch [8/300], Step [101/1725], Loss: 0.2168\n",
            "Epoch [8/300], Step [151/1725], Loss: 0.1143\n",
            "Epoch [8/300], Step [201/1725], Loss: 0.0740\n",
            "Epoch [8/300], Step [251/1725], Loss: 0.0316\n",
            "Epoch [8/300], Step [301/1725], Loss: 0.0183\n",
            "Epoch [8/300], Step [351/1725], Loss: 0.1727\n",
            "Epoch [8/300], Step [401/1725], Loss: 0.1002\n",
            "Epoch [8/300], Step [451/1725], Loss: 0.0096\n",
            "Epoch [8/300], Step [501/1725], Loss: 0.0139\n",
            "Epoch [8/300], Step [551/1725], Loss: 0.0154\n",
            "Epoch [8/300], Step [601/1725], Loss: 0.0216\n",
            "Epoch [8/300], Step [651/1725], Loss: 0.0293\n",
            "Epoch [8/300], Step [701/1725], Loss: 0.3013\n",
            "Epoch [8/300], Step [751/1725], Loss: 0.0363\n",
            "Epoch [8/300], Step [801/1725], Loss: 0.1774\n",
            "Epoch [8/300], Step [851/1725], Loss: 0.0150\n",
            "Epoch [8/300], Step [901/1725], Loss: 0.0506\n",
            "Epoch [8/300], Step [951/1725], Loss: 0.0077\n",
            "Epoch [8/300], Step [1001/1725], Loss: 0.0982\n",
            "Epoch [8/300], Step [1051/1725], Loss: 0.0739\n",
            "Epoch [8/300], Step [1101/1725], Loss: 0.0972\n",
            "Epoch [8/300], Step [1151/1725], Loss: 0.0200\n",
            "Epoch [8/300], Step [1201/1725], Loss: 0.1015\n",
            "Epoch [8/300], Step [1251/1725], Loss: 0.0420\n",
            "Epoch [8/300], Step [1301/1725], Loss: 0.0315\n",
            "Epoch [8/300], Step [1351/1725], Loss: 0.0422\n",
            "Epoch [8/300], Step [1401/1725], Loss: 0.0303\n",
            "Epoch [8/300], Step [1451/1725], Loss: 0.0055\n",
            "Epoch [8/300], Step [1501/1725], Loss: 0.1282\n",
            "Epoch [8/300], Step [1551/1725], Loss: 0.0387\n",
            "Epoch [8/300], Step [1601/1725], Loss: 0.0192\n",
            "Epoch [8/300], Step [1651/1725], Loss: 0.1848\n",
            "Epoch [8/300], Step [1701/1725], Loss: 0.0024\n",
            " Training process of epoch 8 has been done  ------- Loss_train: 0.16170357167720795\n",
            "Epoch [8/300], Step [1/1725], Loss: 1.0542\n",
            "Epoch [8/300], Step [51/1725], Loss: 0.8552\n",
            "Epoch [8/300], Step [101/1725], Loss: 0.7327\n",
            "Epoch [8/300], Step [151/1725], Loss: 1.1834\n",
            "Epoch [8/300], Step [201/1725], Loss: 0.3219\n",
            "Epoch [8/300], Step [251/1725], Loss: 0.7225\n",
            "Epoch [8/300], Step [301/1725], Loss: 0.8388\n",
            "Epoch [8/300], Step [351/1725], Loss: 0.7961\n",
            "Epoch [8/300], Step [401/1725], Loss: 1.1128\n",
            "Epoch [8/300], Step [451/1725], Loss: 1.2003\n",
            "Epoch [8/300], Step [501/1725], Loss: 0.7432\n",
            "Epoch [8/300], Step [551/1725], Loss: 0.2344\n",
            "Epoch [8/300], Step [601/1725], Loss: 0.3231\n",
            "Epoch [8/300], Step [651/1725], Loss: 0.4489\n",
            "Epoch [8/300], Step [701/1725], Loss: 0.1842\n",
            " Validation process of epoch 8 has been done -- F1_score_val: 83.50636774952801% -- Acc_score: 83.42632513314734%\n",
            "Epoch 9/300\n",
            "----------\n",
            "Epoch [9/300], Step [1/1725], Loss: 0.8195\n",
            "Epoch [9/300], Step [51/1725], Loss: 0.4195\n",
            "Epoch [9/300], Step [101/1725], Loss: 0.1578\n",
            "Epoch [9/300], Step [151/1725], Loss: 0.0241\n",
            "Epoch [9/300], Step [201/1725], Loss: 0.0563\n",
            "Epoch [9/300], Step [251/1725], Loss: 0.0201\n",
            "Epoch [9/300], Step [301/1725], Loss: 0.0178\n",
            "Epoch [9/300], Step [351/1725], Loss: 0.1423\n",
            "Epoch [9/300], Step [401/1725], Loss: 0.0960\n",
            "Epoch [9/300], Step [451/1725], Loss: 0.0058\n",
            "Epoch [9/300], Step [501/1725], Loss: 0.0223\n",
            "Epoch [9/300], Step [551/1725], Loss: 0.0174\n",
            "Epoch [9/300], Step [601/1725], Loss: 0.0130\n",
            "Epoch [9/300], Step [651/1725], Loss: 0.0072\n",
            "Epoch [9/300], Step [701/1725], Loss: 0.2386\n",
            "Epoch [9/300], Step [751/1725], Loss: 0.0324\n",
            "Epoch [9/300], Step [801/1725], Loss: 0.1735\n",
            "Epoch [9/300], Step [851/1725], Loss: 0.0275\n",
            "Epoch [9/300], Step [901/1725], Loss: 0.0082\n",
            "Epoch [9/300], Step [951/1725], Loss: 0.0070\n",
            "Epoch [9/300], Step [1001/1725], Loss: 0.0982\n",
            "Epoch [9/300], Step [1051/1725], Loss: 0.1462\n",
            "Epoch [9/300], Step [1101/1725], Loss: 0.0799\n",
            "Epoch [9/300], Step [1151/1725], Loss: 0.0176\n",
            "Epoch [9/300], Step [1201/1725], Loss: 0.0486\n",
            "Epoch [9/300], Step [1251/1725], Loss: 0.0355\n",
            "Epoch [9/300], Step [1301/1725], Loss: 0.0467\n",
            "Epoch [9/300], Step [1351/1725], Loss: 0.0165\n",
            "Epoch [9/300], Step [1401/1725], Loss: 0.0126\n",
            "Epoch [9/300], Step [1451/1725], Loss: 0.0102\n",
            "Epoch [9/300], Step [1501/1725], Loss: 0.0920\n",
            "Epoch [9/300], Step [1551/1725], Loss: 0.0929\n",
            "Epoch [9/300], Step [1601/1725], Loss: 0.0100\n",
            "Epoch [9/300], Step [1651/1725], Loss: 0.1324\n",
            "Epoch [9/300], Step [1701/1725], Loss: 0.0033\n",
            " Training process of epoch 9 has been done  ------- Loss_train: 0.07057153433561325\n",
            "Epoch [9/300], Step [1/1725], Loss: 1.0753\n",
            "Epoch [9/300], Step [51/1725], Loss: 0.7862\n",
            "Epoch [9/300], Step [101/1725], Loss: 0.8957\n",
            "Epoch [9/300], Step [151/1725], Loss: 1.1685\n",
            "Epoch [9/300], Step [201/1725], Loss: 0.3117\n",
            "Epoch [9/300], Step [251/1725], Loss: 0.9119\n",
            "Epoch [9/300], Step [301/1725], Loss: 0.8234\n",
            "Epoch [9/300], Step [351/1725], Loss: 0.8624\n",
            "Epoch [9/300], Step [401/1725], Loss: 1.3752\n",
            "Epoch [9/300], Step [451/1725], Loss: 0.8110\n",
            "Epoch [9/300], Step [501/1725], Loss: 0.7527\n",
            "Epoch [9/300], Step [551/1725], Loss: 0.3072\n",
            "Epoch [9/300], Step [601/1725], Loss: 0.2910\n",
            "Epoch [9/300], Step [651/1725], Loss: 0.3234\n",
            "Epoch [9/300], Step [701/1725], Loss: 0.2237\n",
            " Validation process of epoch 9 has been done -- F1_score_val: 83.22883589941074% -- Acc_score: 83.18961873362076%\n",
            "Epoch 10/300\n",
            "----------\n",
            "Epoch [10/300], Step [1/1725], Loss: 0.8512\n",
            "Epoch [10/300], Step [51/1725], Loss: 0.3832\n",
            "Epoch [10/300], Step [101/1725], Loss: 0.1913\n",
            "Epoch [10/300], Step [151/1725], Loss: 0.1279\n",
            "Epoch [10/300], Step [201/1725], Loss: 0.0885\n",
            "Epoch [10/300], Step [251/1725], Loss: 0.0227\n",
            "Epoch [10/300], Step [301/1725], Loss: 0.0125\n",
            "Epoch [10/300], Step [351/1725], Loss: 0.1287\n",
            "Epoch [10/300], Step [401/1725], Loss: 0.1059\n",
            "Epoch [10/300], Step [451/1725], Loss: 0.0081\n",
            "Epoch [10/300], Step [501/1725], Loss: 0.0209\n",
            "Epoch [10/300], Step [551/1725], Loss: 0.0175\n",
            "Epoch [10/300], Step [601/1725], Loss: 0.0095\n",
            "Epoch [10/300], Step [651/1725], Loss: 0.0056\n",
            "Epoch [10/300], Step [701/1725], Loss: 0.2232\n",
            "Epoch [10/300], Step [751/1725], Loss: 0.0416\n",
            "Epoch [10/300], Step [801/1725], Loss: 0.1420\n",
            "Epoch [10/300], Step [851/1725], Loss: 0.0379\n",
            "Epoch [10/300], Step [901/1725], Loss: 0.0076\n",
            "Epoch [10/300], Step [951/1725], Loss: 0.0087\n",
            "Epoch [10/300], Step [1001/1725], Loss: 0.0741\n",
            "Epoch [10/300], Step [1051/1725], Loss: 0.0969\n",
            "Epoch [10/300], Step [1101/1725], Loss: 0.0775\n",
            "Epoch [10/300], Step [1151/1725], Loss: 0.0076\n",
            "Epoch [10/300], Step [1201/1725], Loss: 0.1205\n",
            "Epoch [10/300], Step [1251/1725], Loss: 0.0547\n",
            "Epoch [10/300], Step [1301/1725], Loss: 0.0692\n",
            "Epoch [10/300], Step [1351/1725], Loss: 0.0337\n",
            "Epoch [10/300], Step [1401/1725], Loss: 0.0047\n",
            "Epoch [10/300], Step [1451/1725], Loss: 0.0063\n",
            "Epoch [10/300], Step [1501/1725], Loss: 0.0961\n",
            "Epoch [10/300], Step [1551/1725], Loss: 0.1397\n",
            "Epoch [10/300], Step [1601/1725], Loss: 0.0271\n",
            "Epoch [10/300], Step [1651/1725], Loss: 0.0868\n",
            "Epoch [10/300], Step [1701/1725], Loss: 0.0007\n",
            " Training process of epoch 10 has been done  ------- Loss_train: 0.0906282514333725\n",
            "Epoch [10/300], Step [1/1725], Loss: 0.9545\n",
            "Epoch [10/300], Step [51/1725], Loss: 0.8111\n",
            "Epoch [10/300], Step [101/1725], Loss: 0.8483\n",
            "Epoch [10/300], Step [151/1725], Loss: 1.2055\n",
            "Epoch [10/300], Step [201/1725], Loss: 0.3766\n",
            "Epoch [10/300], Step [251/1725], Loss: 0.8929\n",
            "Epoch [10/300], Step [301/1725], Loss: 0.9823\n",
            "Epoch [10/300], Step [351/1725], Loss: 0.8983\n",
            "Epoch [10/300], Step [401/1725], Loss: 1.7527\n",
            "Epoch [10/300], Step [451/1725], Loss: 1.1942\n",
            "Epoch [10/300], Step [501/1725], Loss: 0.7923\n",
            "Epoch [10/300], Step [551/1725], Loss: 0.3814\n",
            "Epoch [10/300], Step [601/1725], Loss: 0.2353\n",
            "Epoch [10/300], Step [651/1725], Loss: 0.3943\n",
            "Epoch [10/300], Step [701/1725], Loss: 0.1251\n",
            " Validation process of epoch 10 has been done -- F1_score_val: 83.2813382891521% -- Acc_score: 83.21920703356159%\n",
            "Epoch 11/300\n",
            "----------\n",
            "Epoch [11/300], Step [1/1725], Loss: 0.9095\n",
            "Epoch [11/300], Step [51/1725], Loss: 0.3644\n",
            "Epoch [11/300], Step [101/1725], Loss: 0.0900\n",
            "Epoch [11/300], Step [151/1725], Loss: 0.0124\n",
            "Epoch [11/300], Step [201/1725], Loss: 0.0666\n",
            "Epoch [11/300], Step [251/1725], Loss: 0.0254\n",
            "Epoch [11/300], Step [301/1725], Loss: 0.0173\n",
            "Epoch [11/300], Step [351/1725], Loss: 0.0824\n",
            "Epoch [11/300], Step [401/1725], Loss: 0.1753\n",
            "Epoch [11/300], Step [451/1725], Loss: 0.0753\n",
            "Epoch [11/300], Step [501/1725], Loss: 0.0214\n",
            "Epoch [11/300], Step [551/1725], Loss: 0.0171\n",
            "Epoch [11/300], Step [601/1725], Loss: 0.0119\n",
            "Epoch [11/300], Step [651/1725], Loss: 0.0058\n",
            "Epoch [11/300], Step [701/1725], Loss: 0.1620\n",
            "Epoch [11/300], Step [751/1725], Loss: 0.0241\n",
            "Epoch [11/300], Step [801/1725], Loss: 0.1168\n",
            "Epoch [11/300], Step [851/1725], Loss: 0.0160\n",
            "Epoch [11/300], Step [901/1725], Loss: 0.0067\n",
            "Epoch [11/300], Step [951/1725], Loss: 0.0053\n",
            "Epoch [11/300], Step [1001/1725], Loss: 0.0741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-35cd7814a4f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvsa_bilstm_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-34542b98b7ff>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mloss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQwaes57okUI"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5sf_ABuorJs"
      },
      "source": [
        "INPUT_DIM = len(text_field.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 3\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = text_field.vocab.stoi[text_field.pad_token]\n",
        "CNN_1 = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBTcOzzYphYg"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcAc_wa4pq4r"
      },
      "source": [
        "pretrained_embeddings = text_field.vocab.vectors\n",
        "CNN_1.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzIabVTqCEg"
      },
      "source": [
        "UNK_IDX = text_field.vocab.stoi[text_field.unk_token]\n",
        "\n",
        "CNN_1.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "CNN_1.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmsXzX3pqRId"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "CNN_1 = CNN_1.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvChK9r0q2cc"
      },
      "source": [
        "CNN_1.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KED8C3rLqyNL"
      },
      "source": [
        "train_model(CNN_1, optimizer, \"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LM5Xm1LqkE"
      },
      "source": [
        "vsa_bilstm.cpu()\n",
        "def read_vocab(path):\n",
        "    vocab_file = open(path, \"rb\")\n",
        "    vocab = pickle.load(vocab_file)\n",
        "    return vocab\n",
        "vocab = read_vocab(\"/content/vocab.pkl\")\n",
        "def predict(model, text):\n",
        "    indexed = [int(vocab[token]) if token in vocab.keys() else 0 for token in text]\n",
        "    tensor = torch.LongTensor(indexed).unsqueeze(0)\n",
        "    label = int(torch.argmax(model(tensor),dim = 1).cpu().numpy().item())\n",
        "    return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DYza8n4ikl-"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im8zpZXPh6tc"
      },
      "source": [
        "start = time.time()\n",
        "predict(vsa_bilstm, \"Alo sản_phẩm này sao tệ thế hả\")\n",
        "print(\"the inference time is: \", time.time()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q38IvgREh4eR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}